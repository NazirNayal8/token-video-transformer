{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `MultiheadAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer.attention import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "temporal_dim = 4\n",
    "spatial_dim = 5\n",
    "token_dim = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, temporal_dim, spatial_dim, token_dim))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiheadAttention(token_dim, num_heads)\n",
    "multihead_attention_t = MultiheadAttention(token_dim, num_heads, spatial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = multihead_attention(x)\n",
    "\n",
    "assert y.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = multihead_attention_t(x)\n",
    "\n",
    "assert y.shape == (batch_size, temporal_dim, spatial_dim, token_dim), f\"Got: {y.shape}\"\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Test LateTemporalTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tokenizer.late_temporal_tokenizer import LateTemporalTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "temporal_dim = 4\n",
    "spatial_dim = 5\n",
    "H = 8\n",
    "W = 8\n",
    "C = 4\n",
    "num_frames = 10\n",
    "token_dim = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 64, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, num_frames, H * W, C))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_temporal_tokenizer = LateTemporalTokenizer(C, spatial_dim, temporal_dim, token_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = late_temporal_tokenizer(x)\n",
    "\n",
    "assert y.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `TransformerEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer.encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "temporal_dim = 4\n",
    "spatial_dim = 5\n",
    "token_dim = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, temporal_dim, spatial_dim, token_dim))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(token_dim, spatial=True, num_heads=num_heads)\n",
    "encoder_t = TransformerEncoder(token_dim, spatial=False, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder(x)\n",
    "\n",
    "assert y.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder_t(x)\n",
    "\n",
    "assert y.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `DividedTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer.layer import DividedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "temporal_dim = 4\n",
    "spatial_dim = 5\n",
    "token_dim = 16\n",
    "num_heads = 8\n",
    "layers_1 = [0, 1]\n",
    "layers_2 = [0, 0, 0, 1, 1, 1]\n",
    "layers_3 = [0, 1, 0, 1, 0, 1]\n",
    "layers_4 = [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, temporal_dim, spatial_dim, token_dim))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = DividedTransformer(token_dim, layers_1)\n",
    "t_2 = DividedTransformer(token_dim, layers_2)\n",
    "t_3 = DividedTransformer(token_dim, layers_3)\n",
    "t_4 = DividedTransformer(token_dim, layers_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5, 16])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = t_1(x)\n",
    "y_2 = t_2(x)\n",
    "y_3 = t_3(x)\n",
    "y_4 = t_4(x)\n",
    "\n",
    "assert y_1.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "assert y_2.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "assert y_3.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "assert y_4.shape == (batch_size, temporal_dim, spatial_dim, token_dim)\n",
    "\n",
    "y_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `DividedVideoTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.video_transformer import DividedVideoTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "temporal_dim = 4\n",
    "spatial_dim = 5\n",
    "H = 32\n",
    "W = 32\n",
    "C = 3\n",
    "num_frames = 10\n",
    "token_dim = 16\n",
    "num_heads = 8\n",
    "layers = [0, 1]\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 3, 32, 32])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((batch_size, num_frames, C, H, W))\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "divided_transformer = DividedVideoTransformer(spatial_dim, temporal_dim, token_dim, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = divided_transformer(x)\n",
    "\n",
    "assert y.shape == (batch_size, num_classes)\n",
    "\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
